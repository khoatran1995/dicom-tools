{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import date\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dates(date_str):\n",
    "    if not isinstance(date_str, str):\n",
    "        return pd.NaT  # Return a 'Not a Timestamp' for non-string inputs (likely NaN)\n",
    "    try:\n",
    "        # dateutil's parse function will automatically infer the date format\n",
    "        parsed_date = parser.parse(date_str)\n",
    "        # Convert to pandas Timestamp\n",
    "        timestamp = pd.Timestamp(parsed_date)\n",
    "        # If the time is 00:00:00 (i.e., it's a date only), add a time of 00:00:00\n",
    "        if timestamp.time() == pd.Timestamp('00:00:00').time():\n",
    "            timestamp = timestamp.replace(hour=0, minute=0, second=0)\n",
    "        return timestamp\n",
    "    except ValueError:\n",
    "        return pd.NaT  # Return a 'Not a Timestamp' for unparseable formats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE THE LANGUAGE TRANSLATION DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_map = {\n",
    "    \n",
    "}\n",
    "\n",
    "# define the folder directory\n",
    "base_directory = r'D:\\Khoa Tran\\Work\\04 Xpert results Python tool\\Converting'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS ENGLISH FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully extracted from 1 files, resulting in 4 rows and 27 columns.\n"
     ]
    }
   ],
   "source": [
    "# Function to detect the encoding of a file\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return chardet.detect(f.read())['encoding']\n",
    "\n",
    "# Updated function for extraction focusing on the Detail section\n",
    "def refined_extraction_with_detail(file_path, columns_of_interest, analyte_result_columns, encoding):\n",
    "    within_result_table = False\n",
    "    within_analyte_result_section = False\n",
    "    within_detail_section = False\n",
    "    processed_columns = set()\n",
    "    analyte_result_columns_found = set()\n",
    "\n",
    "    with open(file_path, encoding=encoding) as f:      \n",
    "        for line in f:                                         # run through each line and extract data\n",
    "            line = line.strip()\n",
    "\n",
    "            if \"RESULT TABLE\" in line:\n",
    "                processed_columns = set()\n",
    "                within_analyte_result_section = False\n",
    "\n",
    "            if line in [\"<Insufficient privilege to access data>\", \"\\ufeff\"]:\n",
    "                continue\n",
    "\n",
    "            for header in metadata_sections + [\"RESULT TABLE\", \"Analyte Result\",\"Detail\"]:       #check the header\n",
    "                if header in line:\n",
    "                    if header == \"RESULT TABLE\":\n",
    "                        within_result_table = True\n",
    "                        yield {}\n",
    "                    elif header == \"Analyte Result\":\n",
    "                        within_analyte_result_section = True\n",
    "                        analyte_result_columns_found.clear()\n",
    "                    elif header == \"Detail\":                                                #if it is under Detail, not extract\n",
    "                        within_analyte_result_section = False\n",
    "                    elif header in metadata_sections:\n",
    "                        within_result_table = False\n",
    "                    break\n",
    "\n",
    "            if within_analyte_result_section and line.startswith(tuple(analyte_result_columns)) and \",\" in line:\n",
    "                key, value = line.split(\",\", 1)\n",
    "                key = key.strip()\n",
    "                if key in analyte_result_columns_found:\n",
    "                    continue\n",
    "                analyte_result_columns_found.add(key)\n",
    "                yield {key: value.strip()}\n",
    "\n",
    "            elif within_result_table and not within_analyte_result_section:\n",
    "                if line.startswith(tuple(columns_of_interest)) and \",\" in line:\n",
    "                    key, value = line.split(\",\", 1)\n",
    "                    key = key.strip()\n",
    "                    if key in processed_columns:\n",
    "                        continue\n",
    "                    processed_columns.add(key)\n",
    "                    yield {key: value.strip()}\n",
    "\n",
    "# Function to get all CSV files in a directory and its subdirectories\n",
    "def get_all_csv_files(directory):\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    return csv_files\n",
    "\n",
    "# Process file function\n",
    "def process_file(file_path, columns_of_interest, analyte_result_columns):\n",
    "    try:\n",
    "        encoding = detect_encoding(file_path)\n",
    "        patient_data = {}\n",
    "        for extracted_data in refined_extraction_with_detail(file_path, columns_of_interest, analyte_result_columns, encoding):\n",
    "            if not extracted_data:\n",
    "                if patient_data:\n",
    "                    patient_data[\"file\"] = os.path.basename(file_path)\n",
    "                    all_df_rows.append(patient_data)\n",
    "                    patient_data = {}\n",
    "            else:\n",
    "                patient_data.update(extracted_data)\n",
    "        if patient_data:\n",
    "            patient_data[\"file\"] = os.path.basename(file_path)\n",
    "            all_df_rows.append(patient_data)\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"UnicodeDecodeError encountered while processing file: {file_path}\")\n",
    "\n",
    "# List of metadata sections to ignore\n",
    "metadata_sections = [\"GeneXpert Dx System\", \"ASSAY INFORMATION\", \"Analysis Settings\"]\n",
    "\n",
    "# Define the folder directory\n",
    "base_directory = r'D:\\Khoa Tran\\Work\\04 Xpert results Python tool\\Converting'\n",
    "\n",
    "# List of columns of interest\n",
    "columns_of_interest = [\n",
    "    \"Cartridge S/N\", \"Module Name\", \"Module S/N\",\n",
    "    \"Instrument S/N\", \"S/W Version\", \"Reagent Lot ID\", \"Expiration Date\",\n",
    "    \"Start Time\", \"End Time\", \"Error Status\", \"Status\", \"User\",\n",
    "    \"Sample ID\", \"Patient ID\", \"Assay\", \"Assay Version\",\n",
    "    \"Test Type\", \"Sample Type\", \"Notes\", \"Test Result\"\n",
    "]\n",
    "\n",
    "# Additional columns to be extracted from the Analyte Result section\n",
    "analyte_result_columns = ['SPC', 'IS1081-IS6110', 'rpoB1', 'rpoB2', 'rpoB3', 'rpoB4']\n",
    "\n",
    "\n",
    "# Initialize a list to store all extracted rows\n",
    "all_df_rows = []\n",
    "\n",
    "# Process all files in the directory and its subdirectories\n",
    "csv_files = get_all_csv_files(base_directory)\n",
    "for file_path in csv_files:\n",
    "    process_file(file_path, columns_of_interest, analyte_result_columns)\n",
    "\n",
    "# Create the final DataFrame\n",
    "dfe = pd.DataFrame(all_df_rows)\n",
    "print(f\"Data successfully extracted from {len(csv_files)} files, resulting in {dfe.shape[0]} rows and {dfe.shape[1]} columns.\")\n",
    "dfe.to_excel(r'D:\\Khoa Tran\\Work\\04 Xpert results Python tool\\Converting\\results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS THE FRENCH FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully extracted from 1 files, resulting in 0 rows and 0 columns.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# Function to detect the encoding of a file\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return chardet.detect(f.read())['encoding']\n",
    "\n",
    "# Function for extraction focusing on the Detail section\n",
    "def refined_extraction_with_detail(file_path, columns_of_interest, detail_columns, encoding):\n",
    "    within_result_table = False\n",
    "    within_detail_section = False\n",
    "    processed_columns = set()\n",
    "    detail_columns_found = set()\n",
    "\n",
    "    with open(file_path, encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if \"TABLEAU DE RÉSULTATS\" in line:\n",
    "                processed_columns = set()\n",
    "                within_detail_section = False\n",
    "\n",
    "            if line in [\"<Privilège insuffisant pour accéder aux données>\", \"\\ufeff\"]:\n",
    "                continue\n",
    "\n",
    "            for header in metadata_sections + [\"TABLEAU DE RÉSULTATS\"]:\n",
    "                if header in line:\n",
    "                    if header == \"TABLEAU DE RÉSULTATS\":\n",
    "                        within_result_table = True\n",
    "                        yield {}\n",
    "                    elif header in metadata_sections:\n",
    "                        within_result_table = False\n",
    "                    break\n",
    "\n",
    "            if \"Détail\" in line:\n",
    "                within_detail_section = True\n",
    "                detail_columns_found.clear()\n",
    "                continue\n",
    "\n",
    "            if within_detail_section and line.startswith(tuple(detail_columns)) and \",\" in line:\n",
    "                key, value = line.split(\",\", 1)\n",
    "                key = key.strip()\n",
    "                if key in detail_columns_found:\n",
    "                    continue\n",
    "                detail_columns_found.add(key)\n",
    "                yield {key: value.strip()}\n",
    "\n",
    "            elif within_result_table and not within_detail_section:\n",
    "                if line.startswith(tuple(columns_of_interest)) and \",\" in line:\n",
    "                    key, value = line.split(\",\", 1)\n",
    "                    key = key.strip()\n",
    "                    if key in processed_columns:\n",
    "                        continue\n",
    "                    processed_columns.add(key)\n",
    "                    yield {key: value.strip()}\n",
    "\n",
    "# Function to get all CSV files in a directory and its subdirectories\n",
    "def get_all_csv_files(directory):\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    return csv_files\n",
    "\n",
    "# Process file function\n",
    "def process_file(file_path, columns_of_interest, detail_columns):\n",
    "    try:\n",
    "        encoding = detect_encoding(file_path)\n",
    "        patient_data = {}\n",
    "        for extracted_data in refined_extraction_with_detail(file_path, columns_of_interest, detail_columns, encoding):\n",
    "            if not extracted_data:\n",
    "                if patient_data:\n",
    "                    patient_data[\"file\"] = os.path.basename(file_path)\n",
    "                    all_df_rows.append(patient_data)\n",
    "                    patient_data = {}\n",
    "            else:\n",
    "                patient_data.update(extracted_data)\n",
    "        if patient_data:\n",
    "            patient_data[\"file\"] = os.path.basename(file_path)\n",
    "            all_df_rows.append(patient_data)\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"UnicodeDecodeError encountered while processing file: {file_path}\")\n",
    "\n",
    "# List of metadata sections to ignore\n",
    "metadata_sections = [\"GeneXpert Dx System\", \"INFORMATIONS SUR LE TEST\", \"Paramètres d'analyse\"]\n",
    "\n",
    "# Define the folder directory\n",
    "base_directory = r'C:\\Users\\Admin\\Desktop\\Xpert data processing'\n",
    "\n",
    "# List of columns of interest\n",
    "columns_of_interest = ['Numéro de série de la cartouche', 'Nom du module', 'N° de série du module', \"N° de série de l'instrument\",\n",
    "                        'Version du logiciel', 'N° du lot',  \"Date d'expiration\", 'Heure de lancement',  'Heure de fin',  \"État de l'erreur\",\n",
    "                        'État', 'Utilisateur',  \"N° Id de l'échantillon\",  'N° Id du patient',  'Test',  'Version du test',  'Type de test',\n",
    "                        \"Type d'échantillon\",  'Remarques',  'Résultat du test']\n",
    "\n",
    "# Additional columns to be extracted from the Detail section\n",
    "detail_columns = [\"SPC\", \"IS1081-IS6110\", \"rpoB1\", \"rpoB2\", \"rpoB3\", \"rpoB4\"]\n",
    "\n",
    "# Initialize a list to store all extracted rows\n",
    "all_df_rows = []\n",
    "\n",
    "# Process all files in the directory and its subdirectories\n",
    "csv_files = get_all_csv_files(base_directory)\n",
    "for file_path in csv_files:\n",
    "    process_file(file_path, columns_of_interest, detail_columns)\n",
    "\n",
    "# Create the final DataFrame\n",
    "dff = pd.DataFrame(all_df_rows)\n",
    "print(f\"Data successfully extracted from {len(csv_files)} files, resulting in {dff.shape[0]} rows and {dff.shape[1]} columns.\")\n",
    "\n",
    "# Assuming language_map is a dictionary for column renaming\n",
    "dff.rename(columns=language_map, inplace=True)\n",
    "dff.replace(language_map, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINE AND PROCESS XPERT FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Lists, Dictionaries for replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define list of Pool IDs in Patient ID to be Replaced in Sample ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id_list = [\n",
    "    \"P17 1-1-2-2\",\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary of Pool IDs to be replaceed with the Correct Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of Pool IDs to repalce\n",
    "pool_id_dict = {\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a dictionary of Clean Notes to be replaced with the correct counterpart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a dictionary to further clean the notes field\n",
    "clean_notes_replacement_dict = {\n",
    "\n",
    " \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### qr_code_replacement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_code_replacement_dict = {\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define other Minor Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of Notes to replace with noting\n",
    "notes_to_replace_with_nothing = [\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Combined Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the two data frames together\n",
    "df = pd.concat([dfe,dff])\n",
    "\n",
    "# create a dictionary to rename the columns using partial replacement\n",
    "detail_dic = {\"RÉUSSITE\" : 'PASS',\n",
    "\t\"ÉCHEC\" : 'FAIL',\n",
    "\t'SO,' : 'NA,',\n",
    "\t\"SO\" : 'NA'\n",
    "}\n",
    "# do partial replacement of the detail columns\n",
    "special_columns = ['SPC', 'IS1081-IS6110', 'rpoB1', 'rpoB2', 'rpoB3', 'rpoB4']\n",
    "df[special_columns] = df[special_columns].replace(detail_dic, regex=True)\n",
    "\n",
    "#extract only TB samples\n",
    "df = df[df['Assay']=='Xpert MTB-RIF Ultra']\n",
    "\n",
    "# determine pool IDs that are wrongly placed in the Patient ID field\n",
    "\n",
    "def replace_sample_id_if_match(row, id_list):\n",
    "    # Convert row['Patient ID'] to string and strip any whitespace\n",
    "    patient_id = str(row['Patient ID']).strip()\n",
    "\n",
    "    # Check if the formatted patient_id is in the list\n",
    "    if patient_id in id_list:\n",
    "        return patient_id\n",
    "    else:\n",
    "        return row['Sample ID']\n",
    "\n",
    "# Update the Sample ID with the Patient ID value if it matches a pool ID\n",
    "df['Sample ID'] = df.apply(lambda row: replace_sample_id_if_match(row, patient_id_list), axis=1)\n",
    "\n",
    "# replace values in the SAMPLE ID using the replacement dictionary\n",
    "\n",
    "df['Sample ID'] = df['Sample ID'].replace(pool_id_dict)\n",
    "\n",
    "# define a function to extract the pool ID from the sample ID\n",
    "def extract_pool_id(sample_id):\n",
    "    \"\"\"\n",
    "    This function extracts the potential pool ID from the sample ID.\n",
    "    - It returns None for sample IDs that start with a number from 1 to 6 followed by 3 letters.\n",
    "    - It extracts and returns pool IDs that start with 'P' (case-insensitive) or 'S' followed by 3 digits and a dash.\n",
    "    \"\"\"\n",
    "    if pd.isnull(sample_id) or not isinstance(sample_id, str):\n",
    "        return None\n",
    "    \n",
    "    # Pattern to exclude sample IDs that start with a number from 1 to 6, followed by 3 letters\n",
    "    individual_pattern = r'^[1-6][A-Za-z]{3}'\n",
    "    \n",
    "    # If the sample_id matches the individual_pattern, return None immediately\n",
    "    if re.match(individual_pattern, sample_id):\n",
    "        return None\n",
    "\n",
    "    # Regular expression to capture pool patterns starting with \"P\" (case insensitive) or \"S\" followed by 3 digits and a dash\n",
    "    pool_pattern = r'^(?:[Pp].*|[Ss]\\d{3,}-.*)'\n",
    "    matches = re.match(pool_pattern, sample_id)\n",
    "    if matches:\n",
    "        return matches.group(0)\n",
    "    return None\n",
    "\n",
    "# Extract potential pool IDs and save in a new column 'extracted_pool_ids'\n",
    "df['extracted_pool_ids'] = df['Sample ID'].apply(extract_pool_id)\n",
    "\n",
    "# function to clean the extracted pool IDs\n",
    "def replace_space_in_extracted_id(extracted_id):\n",
    "    \"\"\"\n",
    "    This function cleans up the extracted_pool_ids based on specific patterns.\n",
    "    \"\"\"\n",
    "    if pd.isnull(extracted_id):\n",
    "        return extracted_id\n",
    "\n",
    "    # Pattern that matches a string starting with P or PS followed by at least two digits and then a space\n",
    "    pattern1 = r'^(P|PS)\\d{2,}\\s'\n",
    "    # Pattern that matches a string starting with P or p or PS followed by a space and then at least two digits\n",
    "    pattern2 = r'^(P|p|PS)\\s\\d{2,}'\n",
    "    # Pattern that matches a string starting with P or PS followed by a dash and then at least two digits\n",
    "    pattern3 = r'^(P|PS)-\\d{2,}'\n",
    "\n",
    "    if re.match(pattern1, extracted_id):\n",
    "        extracted_id = extracted_id.replace(\" \", \"-\", 1)\n",
    "    elif re.match(pattern2, extracted_id):\n",
    "        extracted_id = extracted_id.replace(\" \", \"\", 1)\n",
    "    elif re.match(pattern3, extracted_id):\n",
    "        extracted_id = extracted_id.replace(\"-\", \"\", 1)\n",
    "    \n",
    "    # Remove all occurrences of -R and -r\n",
    "    extracted_id1 = extracted_id.replace(\"-R\", \"\").replace(\"-r\", \"\").replace(\"(R)\",\"\").replace(\"RR\",\"\")\n",
    "    \n",
    "    # Replace consecutive dashes with a single dash\n",
    "    extracted_id2 = re.sub(r'-+', '-', extracted_id1)\n",
    "    # clean the extracted id\n",
    "    extracted_id3 = extracted_id2.strip()\n",
    "\n",
    "    # replace = with dash in extracted_id3\n",
    "    extracted_id4 = extracted_id3.replace(\"=\",\"-\")\n",
    "\n",
    "    # replace ; with nothing in extracted_id4\n",
    "    extracted_id5 = extracted_id4.replace(\";\", \"\")\n",
    "    # remove PATIENT DE REDCAP\n",
    "    extracted_id6 = extracted_id5.replace(\"PATIENT DE REDCAP\", \"\")\n",
    "\n",
    "    # Replace the subsequent space with nothing\n",
    "    extracted_id7 = extracted_id6.replace(\" \",\"\")\n",
    "\n",
    "    return extracted_id7\n",
    "\n",
    "\n",
    "# Apply the function to the 'extracted_pool_ids' column\n",
    "df['extracted_pool_ids'] = df['extracted_pool_ids'].apply(replace_space_in_extracted_id)\n",
    "\n",
    "\n",
    "# define a function to remove the left over spaces and other foriegn characters\n",
    "def replace_spaces_and_double_dashes(text):\n",
    "    \"\"\"\n",
    "    Replace all spaces within a string with a dash and then replace all double dashes with a single dash.\n",
    "    \n",
    "    Parameters:\n",
    "    - s (str): Input string\n",
    "    \n",
    "    Returns:\n",
    "    - str: Modified string\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text1 = text.replace(\" \", \"-\")  # Replace spaces with dashes\n",
    "    text2 = text1.replace(\"--\", \"-\")  # Replace double dashes with a single dash\n",
    "    text3 = text2.replace(\"-R\",\"\") # replaece R character\n",
    "    text4 = text3.replace(\"R\",\"\")\n",
    "    text5 = text4.replace(\"=\", \"-\") # Replace equal to sign with dash\n",
    "    text6 = text5.replace(\"-2DJA-\", \"-\")\n",
    "    text7 = text6.strip() # Remove spaces \n",
    "    text8 = text7.replace(\"-()\",\"\")\n",
    "    return text8\n",
    "\n",
    "# Test\n",
    "df['extracted_pool_ids'] = df['extracted_pool_ids'].apply(replace_spaces_and_double_dashes)\n",
    "\n",
    "\n",
    "# DEFINE A FUNCTION FOR CLEANING THE NOTES FIELD\n",
    "def clean_notes(notes):\n",
    "    if pd.isnull(notes) or not isinstance(notes, str):\n",
    "        return notes\n",
    "    \n",
    "    # Remove everything from \"user\" till the end\n",
    "    notes1 = re.sub(r'user.*$', '', notes, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Split on comma and retain the first part\n",
    "    notes2 = notes1.replace(\",\",\" \")\n",
    "    \n",
    "    # Trim leading and trailing spaces\n",
    "    notes3 = notes2.strip()\n",
    "    \n",
    "    # Insert a dash after the three letters for the specified pattern\n",
    "    notes4 = re.sub(r'(?<=[1-6][A-Za-z]{3})(?=\\d{5})', '-', notes3)\n",
    "    \n",
    "    # Replace space with a dash for the specified pattern\n",
    "    notes5 = re.sub(r'(?<=[1-6][A-Za-z]{3})\\s', '-', notes4)\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    notes6 = re.sub(r'\\s+', ' ', notes5)\n",
    "    notes7 = notes6.replace(\"LS:\" , '')\n",
    "    notes8 = notes7.replace(\"LS \" , ' ')    \n",
    "    notes9 = notes8.replace(\" LS \" , ' ')\n",
    "    notes10 = notes9.replace(\"ls \" , ' ')   \n",
    "    notes11 = notes10.replace(\" ls \" , ' ')\n",
    "    notes12 = notes11.replace(\"ls:\" , ' ')\n",
    "    notes13 = notes12.replace(\",\" , ' ')\n",
    "    notes14 = notes13.replace(\"  \" , ' ')\n",
    "    notes15 = notes14.strip()\n",
    "    notes16 = notes15.replace(\".\" , '-')\n",
    "    \n",
    "    return notes16\n",
    "\n",
    "def remove_names_and_dates(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Find the last occurrence of a dash followed by any characters and then a space\n",
    "    pattern = r'-(?:[^-]*?)(\\s)([^-]*$)'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        text = text[:match.start(1)].strip()  # Remove everything from the found space to the end\n",
    "\n",
    "    return text\n",
    "\n",
    "# Applying the function to the 'Notes' column\n",
    "df['clean_notes'] = df['Notes'].apply(clean_notes)\n",
    "\n",
    "# Re-applying the function to the 'clean_notes' column\n",
    "df['clean_notes'] = df['clean_notes'].apply(remove_names_and_dates)\n",
    "\n",
    "# Replace specified notes with NaN\n",
    "df['clean_notes'] = df['clean_notes'].replace(notes_to_replace_with_nothing, np.nan)\n",
    "\n",
    "# Replace 'extracted_pool_ids' with NaN where 'Notes' field is blank and 'extracted_pool_ids' is not null\n",
    "df.loc[(df['clean_notes'] == '') & (df['extracted_pool_ids'].notnull()), 'extracted_pool_ids'] = np.nan\n",
    "\n",
    "# replace the cleaned notes with the dictionary\n",
    "df['clean_notes'] = df['clean_notes'].replace(clean_notes_replacement_dict)\n",
    "\n",
    "# drop the rows where extracted_pool_ids are not blank but the clean notes is blank\n",
    "# Condition to drop rows\n",
    "condition = (df['extracted_pool_ids'].notna()) & (df['clean_notes'].isna())\n",
    "\n",
    "# Dropping rows that meet the condition\n",
    "df = df.drop(df[condition].index)\n",
    "\n",
    "#calculate the pool size by counting the number of spaces in the clean notes field\n",
    "def count_spaces_in_notes(notes, extracted_pool_ids):\n",
    "    \"\"\"\n",
    "    Count the number of spaces in the notes field after cleaning and trimming,\n",
    "    but only if extracted_pool_ids is not null.\n",
    "\n",
    "    :param notes: The notes field (string).\n",
    "    :param extracted_pool_ids: The extracted pool IDs field.\n",
    "    :return: The number of spaces in the cleaned and trimmed notes field if extracted_pool_ids is not null, otherwise 0.\n",
    "    \"\"\"\n",
    "    if pd.isnull(notes) or pd.isnull(extracted_pool_ids):\n",
    "        return 0\n",
    "\n",
    "    # Clean and trim the notes field\n",
    "    cleaned_notes = notes.strip()\n",
    "\n",
    "    # Replace double spaces with a single space\n",
    "    while \"  \" in cleaned_notes:\n",
    "        cleaned_notes = cleaned_notes.replace(\"  \", \" \")\n",
    "\n",
    "    # Count the number of spaces\n",
    "    return cleaned_notes.count(\" \")\n",
    "\n",
    "\n",
    "# Apply the function to create a new column with the number of spaces\n",
    "df['pool_size'] = df.apply(lambda x: count_spaces_in_notes(x['clean_notes'], x['extracted_pool_ids']), axis=1) +1\n",
    "\n",
    "#update the pool size to 1 if the clean notes field is empty\n",
    "def set_pool_size(row):\n",
    "    if pd.isnull(row['clean_notes']) or pd.isnull(row['extracted_pool_ids']):\n",
    "        return 1\n",
    "    else:\n",
    "        return row['pool_size']\n",
    "\n",
    "# Apply the function to each row\n",
    "df['pool_size'] = df.apply(set_pool_size, axis=1)\n",
    "\n",
    "# create a column is_pool if pool size is greater than 1\n",
    "df['is_pool'] = df['pool_size'] > 1\n",
    "\n",
    "\n",
    "# classify results into groups\n",
    "error_results = ['ERROR', 'NO RESULT', 'INVALID']\n",
    "\n",
    "#positive result groups\n",
    "positive_results = [\n",
    "        'MTB DETECTED MEDIUM||RIF Resistance NOT DETECTED',\n",
    "        'MTB DETECTED HIGH||RIF Resistance NOT DETECTED', \n",
    "        'MTB DETECTED LOW||RIF Resistance NOT DETECTED',\n",
    "        'MTB DETECTED VERY LOW||RIF Resistance NOT DETECTED',\n",
    "        'MTB DETECTED MEDIUM||RIF Resistance DETECTED',\n",
    "        'MTB DETECTED LOW||RIF Resistance DETECTED',\n",
    "        'MTB DETECTED HIGH||RIF Resistance DETECTED',\n",
    "        '|MTB Trace DETECTED|RIF Resistance INDETERMINATE', \n",
    "        'MTB DETECTED VERY LOW||RIF Resistance DETECTED',\n",
    "        'MTB DETECTED LOW||RIF Resistance INDETERMINATE',\n",
    "        'MTB DETECTED HIGH||RIF Resistance INDETERMINATE',\n",
    "        'MTB DETECTED VERY LOW||RIF Resistance INDETERMINATE',\n",
    "        'MTB Trace DETECTED|RIF Resistance INDETERMINATE',\n",
    "        'MTB DETECTED MEDIUM||RIF Resistance INDETERMINATE',\n",
    "        'MTB DETECTED VERY LOW||RIF Resistance INDETERMINATE',\n",
    "        'MTB DETECTED HIGH||RIF Resistance INDETERMINATE',\n",
    "        'MTB DETECTED MEDIUM||RIF Resistance DETECTED',\n",
    "        'MTB DETECTED LOW||RIF Resistance INDETERMINATE',\n",
    "       \n",
    "       \n",
    "       ]\n",
    "\n",
    "#negative result groups\n",
    "negative_results = ['MTB NOT DETECTED||']\n",
    "\n",
    "#function to classify the test results\n",
    "def classify_test_result(row):\n",
    "    # Check the 'is_pool' value of the current row\n",
    "    if row['is_pool'] == False:\n",
    "        return 'individual'\n",
    "\n",
    "    if row['Test Result'] in error_results:\n",
    "        return 'error'\n",
    "    elif row['Test Result'] in positive_results:\n",
    "        return 'positive'\n",
    "    elif row['Test Result'] in negative_results:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "# Applying the function to the DataFrame\n",
    "df['pool_result'] = df.apply(classify_test_result, axis=1)\n",
    "\n",
    "# segment the data frames by pools\n",
    "df_pool = df[df['is_pool']==True]\n",
    "df_individual = df[df['is_pool']==False]\n",
    "\n",
    "# make a copy of the clean notes field\n",
    "df_pool['clean_notes_exploded'] = df_pool['clean_notes']\n",
    "\n",
    "# define a function to explode the dataframe using the Clean Notes field\n",
    "def explode_dataframe(df_pool):\n",
    "    # Split the 'clean_notes' column by space and explode it\n",
    "    df_exploded = df_pool.assign(clean_notes=df_pool['clean_notes'].str.split(' ')).explode('clean_notes')\n",
    "    return df_exploded\n",
    "df_pool = explode_dataframe(df_pool)\n",
    "\n",
    "df = pd.concat([df_pool, df_individual])\n",
    "\n",
    "\n",
    "# define a function to further clean the Notes field to enable the extraction of the QR Code\n",
    "def further_clean_notes(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Handle the format without a dash\n",
    "    pattern1 = r'([1-6][A-Za-z]{3})(\\d{1,4})(?!\\d|-)'\n",
    "    replacement1 = lambda m: m.group(1) + '-' + m.group(2).zfill(5)\n",
    "    text = re.sub(pattern1, replacement1, text)\n",
    "\n",
    "    # Handle the format with a dash but less than 5 digits after the dash\n",
    "    pattern2 = r'([1-6][A-Za-z]{3}-)(\\d{1,4})(?!\\d)'\n",
    "    replacement2 = lambda m: m.group(1) + m.group(2).zfill(5)\n",
    "    text2 = re.sub(pattern2, replacement2, text)\n",
    "\n",
    "    # Handle the format with 6 digits after the dash, where the first digit is 0\n",
    "    pattern3 = r'([1-6][A-Za-z]{3}-)0(\\d{5})'\n",
    "    replacement3 = lambda m: m.group(1) + m.group(2)\n",
    "    text3 = re.sub(pattern3, replacement3, text2)\n",
    "\n",
    "    return text3\n",
    "\n",
    "#clean the clean notes fied to extract qr codes\n",
    "df['clean_notes'] = df['clean_notes'].apply(further_clean_notes)\n",
    "df['clean_notes'] = df['clean_notes'].apply(further_clean_notes)\n",
    "df['Sample ID'] = df['Sample ID'].apply(further_clean_notes)\n",
    "df['Patient ID'] = df['Patient ID'].apply(further_clean_notes)\n",
    "\n",
    "df['clean_notes'] = df['clean_notes'].str.upper()\n",
    "df['Sample ID'] = df['Sample ID'].str.upper()\n",
    "df['Patient ID'] = df['Patient ID'].str.upper()\n",
    "\n",
    "# Replace the wrongly typed qr code with the qr_code_replacement_dict\n",
    "df['clean_notes'] = df['clean_notes'].replace(qr_code_replacement_dict)\n",
    "\n",
    "# do the replacement in Sample ID field\n",
    "df['Sample ID'] = df['Sample ID'].replace(qr_code_replacement_dict)\n",
    "\n",
    "# do the replacement in Patient ID field\n",
    "df['Patient ID'] = df['Patient ID'].replace(qr_code_replacement_dict)\n",
    "\n",
    "# define a function to extract the QR Code\n",
    "def extract_qr_codes(df):\n",
    "    # Define the regex pattern for a QR code\n",
    "    qr_pattern = r'([1-6][A-Z]{3}-\\d{5})'\n",
    "    \n",
    "    def get_qr_code(row):\n",
    "        # First attempt to extract from the 'clean_notes' column if 'is_pool' is True\n",
    "        if row['is_pool']:\n",
    "            match = re.search(qr_pattern, str(row['clean_notes']))\n",
    "            if match:\n",
    "                return match.group(0)\n",
    "        \n",
    "        # If not found in 'clean_notes' or 'is_pool' is False, try the 'Sample ID' column\n",
    "        match = re.search(qr_pattern, str(row['Sample ID']))\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "        \n",
    "        # If not found in 'Sample ID', try the 'Patient ID' column\n",
    "        match = re.search(qr_pattern, str(row['Patient ID']))\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "        \n",
    "        return None  # If no QR code found in any of the fields\n",
    "\n",
    "    # Extract QR codes based on the function\n",
    "    df['qr_codes'] = df.apply(get_qr_code, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = extract_qr_codes(df)\n",
    "\n",
    "\n",
    "# Function to extract a QR code from a single row\n",
    "def get_qr_code2(row):\n",
    "    qr_pattern = r'([1-6][A-Z]{3}-\\d{5})'\n",
    "\n",
    "    # Check 'Sample ID' for a QR code\n",
    "    match = re.search(qr_pattern, str(row['Sample ID']))\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    \n",
    "    # If not found, check 'Patient ID'\n",
    "    match = re.search(qr_pattern, str(row['Patient ID']))\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "\n",
    "    # If still not found, check 'clean_notes'\n",
    "    match = re.search(qr_pattern, str(row['clean_notes']))\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    \n",
    "    return None  # Return None if no QR code found\n",
    "\n",
    "# Update 'qr_codes' where it is currently null\n",
    "mask = df['qr_codes'].isnull()\n",
    "df.loc[mask, 'qr_codes'] = df[mask].apply(get_qr_code2, axis=1)\n",
    "\n",
    "# Now df contains updated 'qr_codes' values\n",
    "# define a function to extract the S4A QR Code\n",
    "def extract_s4a_qr_codes(row, columns_to_search):\n",
    "    \"\"\"\n",
    "    Extract the first S4A QR code from specified columns of a DataFrame row.\n",
    "\n",
    "    :param row: A row of the DataFrame.\n",
    "    :param columns_to_search: List of column names to search for the QR code.\n",
    "    :return: The first extracted S4A QR code or NaN if none found.\n",
    "    \"\"\"\n",
    "    pattern = r'SC[1-5]-\\d{4}'\n",
    "\n",
    "    for column in columns_to_search:\n",
    "        if not isinstance(row[column], str):\n",
    "            continue\n",
    "\n",
    "        match = re.search(pattern, row[column])\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# Define columns to search for QR codes\n",
    "columns_to_search = ['clean_notes', 'Patient ID', 'Sample ID']\n",
    "\n",
    "# Apply the function to create a new column with the extracted S4A QR codes\n",
    "df['s4a'] = df.apply(extract_s4a_qr_codes, args=(columns_to_search,), axis=1)\n",
    "\n",
    "# Define a function to determine the project based on s4a and qr_codes\n",
    "def determine_project(row):\n",
    "    if pd.notnull(row['s4a']):\n",
    "        return 'S4A'\n",
    "    elif pd.notnull(row['qr_codes']):\n",
    "        return 'Inspire'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the function to create a new column 'project'\n",
    "df['project'] = df.apply(determine_project, axis=1)\n",
    "\n",
    "# Update 'qr_codes' to the value of 's4a' if 'qr_codes' is null\n",
    "df['qr_codes'] = df.apply(lambda row: row['s4a'] if pd.isnull(row['qr_codes']) else row['qr_codes'], axis=1)\n",
    "\n",
    "# define a function to update the qr code in place\n",
    "def update_qr_codes_inplace(df):\n",
    "    \"\"\"\n",
    "    Update the qr_codes column in the DataFrame in place.\n",
    "    If is_pool is True and qr_codes is null, set qr_codes to clean_notes.\n",
    "    Otherwise, set qr_codes to Sample ID where qr_codes is null.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to be updated.\n",
    "    \"\"\"\n",
    "    # Compute the values to be assigned\n",
    "    values_to_assign = np.where(\n",
    "        df['is_pool'] & df['qr_codes'].isnull(), \n",
    "        df['clean_notes'], \n",
    "        df['Sample ID']\n",
    "    )\n",
    "\n",
    "    # Filter the values to match the length of rows where 'qr_codes' is null\n",
    "    values_to_assign = values_to_assign[df['qr_codes'].isnull()]\n",
    "\n",
    "    # Assign the computed values to 'qr_codes' where 'qr_codes' is null\n",
    "    df.loc[df['qr_codes'].isnull(), 'qr_codes'] = values_to_assign\n",
    "\n",
    "update_qr_codes_inplace(df)\n",
    "\n",
    "# Merge the xpert files with the main dataframe\n",
    "df = df.merge(xpert_files[['file', 'lab', 'start_date', 'end_date', 'date_created']], on = 'file', how = 'left')\n",
    "\n",
    "# Convert 'date_created' to datetime format\n",
    "df['date_created'] = pd.to_datetime(df['date_created'])\n",
    "\n",
    "# Sort by 'Cartridge S/N' and 'date_created' in descending order\n",
    "df = df.sort_values(by=['Cartridge S/N', 'date_created'], ascending=[True, False])\n",
    "\n",
    "# Group by 'Cartridge S/N' and keep only the rows with the latest 'date_created' for each group\n",
    "df = df[df['date_created'] == df.groupby('Cartridge S/N')['date_created'].transform('max')]\n",
    "\n",
    "# sort the dataframe by the pool result, pool size, and qr codes\n",
    "df.sort_values(by =['is_pool','Cartridge S/N', 'file'], ascending=[False, True, True], inplace = True)\n",
    "df = df.drop_duplicates(subset = ['Cartridge S/N','qr_codes'])\n",
    "df.dropna(subset = ['qr_codes'], inplace = True)\n",
    "\n",
    "# Define the desired categories order\n",
    "categories_order = ['positive', 'negative', 'individual', 'error']\n",
    "\n",
    "# Convert the 'pool_result' column to categorical with the specified order\n",
    "df['pool_result'] = pd.Categorical(df['pool_result'], categories=categories_order, ordered=True)\n",
    "\n",
    "# Convert the 'pool_result' column to categorical with the specified order\n",
    "df['pool_result'] = pd.Categorical(df['pool_result'], categories=categories_order, ordered=True)\n",
    "\n",
    "#drop duplicates\n",
    "df.drop_duplicates(subset = ['Cartridge S/N', 'qr_codes'], inplace = True)\n",
    "\n",
    "df.sort_values(by=['date_created','is_pool', 'pool_result', 'qr_codes',], ascending=[False, False, True, True,], inplace=True)\n",
    "\n",
    "#determine the duplicate number for each qr code\n",
    "df['dup_no'] = df.groupby('qr_codes').cumcount() + 1\n",
    "# Sort the dataframe\n",
    "df.sort_values(by=['pool_result', 'qr_codes', 'dup_no'], ascending=[True, True, True], inplace=True)\n",
    "\n",
    "#Write a function to get the individual results\n",
    "def get_individual_values(row, df):\n",
    "    # Check if the row is a positive pooled sample\n",
    "    if row['is_pool'] and row['pool_result'] == 'positive':\n",
    "        # Create a subset DataFrame for individual tests matching the QR code of the pool\n",
    "        subset_df = df[(df['qr_codes'] == row['qr_codes']) & (~df['is_pool'])]\n",
    "        # If individual tests are found, return the Test Result of the first one\n",
    "        if not subset_df.empty:\n",
    "            return subset_df['Test Result'].iloc[0]\n",
    "        # If no individual tests are found, return \"No Individual Results Yet\"\n",
    "        else:\n",
    "            return \"No Ind Test Yet\"\n",
    "    # For all other cases, return the Test Result of the row\n",
    "    return row['Test Result']\n",
    "\n",
    "\n",
    "# Apply the function to the 'Test Result' column of df_valid\n",
    "df['individual_results'] = df.apply(lambda row: get_individual_values(row, df), axis=1)\n",
    "\n",
    "# function to get the individual cartridge SN\n",
    "def get_individual_cartridge_sn(row):\n",
    "    # If the row represents a pool and the result is positive\n",
    "    if row['is_pool'] and row['pool_result'] == 'positive':\n",
    "        # Filter df to find the corresponding row where 'is_pool' is False\n",
    "        subset_df = df[(df['qr_codes'] == row['qr_codes']) & (~df['is_pool'])]\n",
    "\n",
    "        # If there's a match, return the 'Cartridge S/N' value from the subset\n",
    "        if not subset_df.empty:\n",
    "            return subset_df['Cartridge S/N'].iloc[0]\n",
    "        # If no individual test is found, return \"No Individual Results Yet\"\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # For all other cases (including pools that are not positive), return None\n",
    "    return None\n",
    "\n",
    "df['individual_cartridge_sn'] = df.apply(lambda row: get_individual_cartridge_sn(row), axis=1)\n",
    "\n",
    "# Define the categories based on individual results\n",
    "def categorize_result(individual_result):\n",
    "    if individual_result in positive_results:\n",
    "        return 'positive'\n",
    "    elif individual_result in negative_results:\n",
    "        return 'negative'\n",
    "    elif individual_result in error_results:\n",
    "        return 'invalid'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "df['indiv_result_category'] = df['individual_results'].apply(categorize_result)\n",
    "\n",
    "\n",
    "#create the final pool size for individuals where the pool was positive\n",
    "def update_pool_size(df):\n",
    "    # Create a new column 'final_pool_size' with the same values as 'pool_size' initially\n",
    "    df['final_pool_size'] = df['pool_size']\n",
    "    \n",
    "    # Find the unique xpert_ids with more than one occurrence\n",
    "    xpert_ids = df['qr_codes'].value_counts()\n",
    "    xpert_ids = xpert_ids[xpert_ids > 1].index.tolist()\n",
    "    \n",
    "    # Iterate through each unique xpert_id\n",
    "    for xpert_id in xpert_ids:\n",
    "        # Check if xpert_id has both is_pool True and False entries\n",
    "        if df[(df['qr_codes'] == xpert_id) & (df['is_pool'])].empty or df[(df['qr_codes'] == xpert_id) & (~df['is_pool'])].empty:\n",
    "            continue  # Skip ids that don't have both True and False entries\n",
    "        \n",
    "        # Get the pool size where is_pool is True\n",
    "        true_pool_size = df[(df['qr_codes'] == xpert_id) & (df['is_pool'])]['pool_size'].iloc[0]\n",
    "        \n",
    "        # Update the 'final_pool_size' where is_pool is False for the same xpert_id\n",
    "        df.loc[(df['qr_codes'] == xpert_id) & (~df['is_pool']), 'final_pool_size'] = true_pool_size\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Now call the function to update the DataFrame\n",
    "df = update_pool_size(df) \n",
    "\n",
    "# if final pool size  = 1, then test type = individual else pooled\n",
    "df['test_type'] = np.where(df['final_pool_size']==1, 'individual', 'pooled')\n",
    "\n",
    "# Define the QR code pattern\n",
    "qr_pattern = r'([1-6][A-Z]{3}-\\d{5})'\n",
    "\n",
    "# Function to extract the first four characters if qr_code matches the pattern\n",
    "def extract_site(qr_code):\n",
    "    if re.match(qr_pattern, qr_code):\n",
    "        return qr_code[:4]\n",
    "    else:\n",
    "        return None  # Or return '' if you prefer an empty string for non-matches\n",
    "\n",
    "# Create the 'site' column based on 'qr_codes'\n",
    "df['site'] = df['qr_codes'].apply(extract_site)\n",
    "\n",
    "# determine if someone has TB\n",
    "df['has_tb'] = df['indiv_result_category'] == 'positive'\n",
    "\n",
    "# where site is not null, xpert_id = qr_codes\n",
    "df['xpert_id'] = np.where(df['site'].notnull(), df['qr_codes'], np.nan)\n",
    "\n",
    "# if both extracted_pool_ids and Notes are notnull, then pool_id = extracted_pool_ids\n",
    "df['pool_id'] = np.where(df['extracted_pool_ids'].notnull() & df['Notes'].notnull(), df['extracted_pool_ids'], np.nan)\n",
    "\n",
    "\n",
    "# convert datetime columns to datetime\n",
    "df['Expiration Date'] = df['Expiration Date'].apply(fix_dates)\n",
    "df['Start Time'] = df['Start Time'].apply(fix_dates)\n",
    "df['End Time'] = df['End Time'].apply(fix_dates)\n",
    "# Define the date for comparison\n",
    "date_threshold = pd.Timestamp('2023-06-27')\n",
    "current_date = pd.Timestamp.today()\n",
    "\n",
    "# Create the 'date' column based on the conditions\n",
    "df['date'] = np.where(\n",
    "    df['End Time'] < date_threshold, df['start_date'],\n",
    "    np.where(df['End Time'] > current_date, df['end_date'], df['End Time']))\n",
    "\n",
    "#fill missing values in the date column with the enddate value\n",
    "df['date'] = df['date'].fillna(df['end_date'])\n",
    "\n",
    "def classify_pool(group):\n",
    "    \"\"\"\n",
    "    Classify the pool based on individual test results within the group.\n",
    "    \"\"\"\n",
    "    if group['pool_result'].iloc[0] == 'error':\n",
    "        return 'Error'\n",
    "    elif group['pool_result'].iloc[0] == 'negative':\n",
    "        return 'Negative'\n",
    "    elif group['pool_result'].iloc[0] == 'positive':\n",
    "        if all(group['indiv_result_category'] == 'negative'):\n",
    "            return 'False Positive'\n",
    "        elif any(group['indiv_result_category'] == 'positive'):\n",
    "            return 'True Positive'\n",
    "        elif all((group['indiv_result_category'] == 'negative') | (group['indiv_result_category'] == 'unknown')):\n",
    "            return 'Incomplete'\n",
    "    return 'Unknown'\n",
    "\n",
    "# Filter the DataFrame for pooled samples and apply the classification function to each pooled group\n",
    "pooled_df = df[df['is_pool']]\n",
    "pooled_classifications = pooled_df.groupby('Cartridge S/N').apply(classify_pool)\n",
    "\n",
    "# Map the classifications back to the original DataFrame for pooled samples\n",
    "df.loc[df['is_pool'], 'pool_classification'] = df['Cartridge S/N'].map(pooled_classifications)\n",
    "\n",
    "# Set pool_classification to NaN where final_pool_size is 1\n",
    "df.loc[df['final_pool_size'] == 1, 'pool_classification'] = np.nan\n",
    "\n",
    "#drop unnecessary columns\n",
    "df.drop(columns= ['rpoB1 melt', 'rpoB2 melt', 'rpoB3 melt', 'rpoB4 melt', 'rpoB1 Mut melt', 'rpoB2 Mut melt', 'rpoB3 Mut melt',\n",
    "       'rpoB4 Mut melt A', 'rpoB4 Mut melt B', 'Patient ID 2', 'Assay', 'Assay Version', 'Assay Type', 'Test Type',\n",
    "       'clean_notes', 'Module Name', 'start_date' , 's4a', 'start_date', 'end_date', 'date_created', 'Sample Type', 'clean_notes',], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
